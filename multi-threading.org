#+INCLUDE: theme/style.org 
#+TITLE: Multithreading and concurrency 
#+DESCRIPTION: cpp/c++ thread concurrency std::thread modern cpp c++ 
#+AUTHOR:      Caio Rodrigues Soares - caiorss [DOT] rodrigues [AT] gmail [DOT] COM
#+STARTUP: content 

* Multithreading and concurrency 
** Fundamental Concepts 

 *Processes and Threads*

Processes: 

 + A process is a running program with its own virtual memory,
   address space, unique process identifier ID and context (CPU
   registers - IP - Instruction Pointer and SP - Stack Pointer)

 + A single _CPU core_ is only capable of executing a single process
   at a time. However, users have the illusion that multiple
   processes are being run simultaneously because the _operating system's_
   _scheduler_ multiplexes the CPU execution time between all
   processes. As a result, each process is run sequentially by a
   single CPU core during a short _time slice_ (time sharing). When the
   scheduler switches to another process, it saves the _process' state_
   (current directory, CPU registers, ...) and loads the state of the
   next process (context switching). 

Threads: 

  * A thread is a independent flow of execution or task within a
    single process. The purpose of threads is to allow a process to
    execute multiple independent simultaneous tasks, such as running
    the user interface dispatch thread, handling socket connections,
    performing the download in a different thread and so on.

  * For an operating system, a thread is a _lightweight process_ or a
    stripped down process with its own stack, local data, CPU
    registers (specially IP - Instruction Pointer and SP - Stack
    Pointer), but without its own address space and virtual memory,
    instead it only can access to the virtual memory of the process
    that thread belongs to.

  * Threads are also known as (aka):
    + Lightweight process 
    + Native thread 
    + Kernel-thread
    + Operating system thread or OS thread 

Benefits of multi-threading:

  + Increase application responsiviness, specially GUI - Graphical
    User Interface Applications.

  + Take advantage of multi-core processor

  + Speed up heavy math computations. Multi-threading allows to
    split a heavy matrix calculation into multiple threads running
    in different CPU cores. 

  + Fewer system resources usage. Using multiple threads for running
    multiple tasks is cheaper than running multiple processes for
    each task. Note: before multi-threading, it was common in
    Unix-like operating system to use the _fork()_ system call to fork
    (copy) the current process for handling client socket
    connections in network server appliocations.  


Synchronization Primitives

  + Mutex

  + Semaphore

  + Condition variables

  + Barriers 

  + Atomic variables

Potential Problems of Concurrency and Multi-threading

  + Race condition, aka data race

  + Deadlock

  + Starvation 

  + Oversubscription

  + Load balancing

  + Thread exaustion 

  *Lowest Level Threading APIs:*

Threads and processes require operating system support, therefore
those APIs are operating system specific. The most fundamental APIs
provided by operating system for accessing threads are: 

  * _pthread (Posix threads)_ - POSIX API (C-API)
    * The posix thread API is implemented by most Unix-like operating
      systems such Linux, MacOsx, iOS, Android, BSD and some embedded
      real time systems such as RTEMS, QNX and VxWorks.

  * _Windows Win32 Thread API_


  *Hardware* 
 
  + Physical Processor, aka CPU (Central Processing Unit) or socket
    + => Chip visible in the computer's motherboard. A single modern
      chip can contain multiple processing units inside of it, called
      _CPU core_. Note: some _server computers_ may have multiple physical
      processors or CPU chips.

  + CPU Core and Multicore CPUs 

    + => A CPU core is a _processing unit_. Nowadays most processors
      chips are _multicore_, a single CPU chip contain multiple cores
      within a single unit. A CPU with N cores is capable of executing
      at least N stream of instruction simultaneously or at least N
      _hardware threads_.

  + Hyper Thread
    + => _Hyper threading_ is a Intel's proprietary technology which
      allows a single CPU core to run process multiple streams of
      instructions as it was multiple processors. In other words, a
      single core is capable of running multiple threads in parallel. 

  + Total number of hardware threads or _logical processors_
    + => NHW = TOTAL NUMBER OF LOGICAL PROCESSORS = TOTAL NUMBER OF THREADS

#+BEGIN_SRC text 
   NHW = (Number of CPUs) * (Number of Cores per CPU) * ( Number of HW threads per core ) 
#+END_SRC

A server computer with 2 physical processors or CPU sockets, 4
processing cores per CPU and 2 threads per core has a total of 

 + NHW = 2 x 2 x 4 = 16 threads or 16 logical processors 

** Standard Library Reference 
 
 *Technical Specifications* 

  + P0159 - Technical Specifiction for Concurrency

  + P0024 - Technical Specifiction for Parallelism

  + C++17 Concurrency TS (Technical Specification)
 

 *C++11/14 Thread API*

   + std::threads (C++11)

 *Task Based API*

  + [[https://docs.microsoft.com/en-us/cpp/standard-library/promise-class?view=vs-2019][std::promise]] (Microsoft) 

  + [[https://docs.microsoft.com/en-us/cpp/standard-library/future?view=vs-2019][std::future]] (Microsoft)

  + std::future<>, std::shared_future<>, std::atomic_future<> 

  + [[https://docs.microsoft.com/en-us/cpp/standard-library/packaged-task-class?view=vs-2019][std::packaged_task]] (Microsft)

  + std::async

  + Function: [[https://docs.microsoft.com/en-us/cpp/standard-library/future-functions?view=vs-2019][std::async]] (Microsft)

  + std::launch 

C++17 Additions: 

 + Concurrent TS (Nonblocking futures (.then), executors, await)
 + future::when_any
 + future::when_all()
 + future::then() 
 + future::unwrap()

 *Syncronization Primitives:*

 + Locks 
   + [[https://en.cppreference.com/w/cpp/thread/mutex][std::mutex]]
   + std::condition_variable

 + RAII Wrappers for locks 
   + [[https://en.cppreference.com/w/cpp/thread/unique_lock][std::unique_lock<>]] 
   + [[https://en.cppreference.com/w/cpp/thread/lock_guard][std::lock_guard]] - RAII Wrapper for locks

 + Atomic Operations => Header: [[https://en.cppreference.com/w/c/atomic][<atomic>]]
   + [[https://en.cppreference.com/w/cpp/atomic/atomic][std::atomic]] 
   + std::atomic_xxx, std::atomic<>, std::atomic_thread_fence()

 *Implementations of C++11 Standard Library*

   + Clang LLVM 

   + GNU GCC/G++  

   + MSVC - Microsft Visual C++ Compiler (aka Visual Studio Compiler)

   + just::thread - commercial implementation by Just Software
     Solution for MSVC, GNU GCC/G++ and CLang.

** Class std::thread 

The class std::threads _is not thread_, it is a proxy for a native
thread and encapsulates a native thread or a kernel thread which the
documentation calls _thread of execution_.  

Header: 
  + [[https://en.cppreference.com/w/cpp/header/thread][<thread>]]

Documentation: 

 + [[https://en.cppreference.com/w/cpp/thread/thread/thread][std::thread]]  - cppreference

 + [[https://docs.microsoft.com/en-us/cpp/standard-library/thread-class?view=vs-2019][std::thread]] - Microsft MSFT, MSVC

 + [[https://www.boost.org/doc/libs/1_71_0/doc/html/thread.html][boost:thread]]  - Predecessor of the standard library threads

 + [[https://www.boost.org/doc/libs/1_71_0/doc/html/thread/thread_management.html][Thread Management - 1.71.0]] (Boost docs)
 
Papers related to the standard library implementation: 

 * [[http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2006/n2094.html][WG21 - N2093]]  - Multithreading API for C++0X - A Layered Approach - 2006-09-09

 * [[http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2006/n2139.html][WG21 - N2139]] - Thoughts on a Thread Library for C++ - 2006-11-06

 * [[http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2184.html][WG21 - N2184]] - Thread Launching for C++0X - 2007-03-09

 * [[http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2497.html][WG21 - N2497]] - Multi-threading Library for Standard C++ (Revision 1) - 2008-01-07

 * [[http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2320.html][WG21 - N2320]] - Multi-threading Library for Standard C++ - 2007-06-24


 *Type of signature of std::thread member functions*

#+BEGIN_SRC cpp 
  class thread
  {
  public:
      // types:
      class id;
      // typedef implementation-defined native_handle_type; // See [thread.native]

      // construct/copy/destroy:
      thread();

      template <class F> explicit thread(F f);

      template <class F, class ...Args> thread(F&& f, Args&&... args);

      ~thread();

      thread(const thread&) = delete;

      thread(thread&&);

      thread& operator=(const thread&) = delete;

      thread& operator=(thread&&);

      // members:
      void swap(thread&&);
      bool joinable() const;
      void join();
      void detach();
      id get_id() const;
      native_handle_type native_handle(); // See [thread.native]

      // static members:
      static unsigned hardware_concurrency();
  };
#+END_SRC

 *Detailed Member Functions of class std::thread*

 + _Default and move constructors_ 

#+BEGIN_SRC cpp 
  // Default constructor - without any thread of execution 
  thread() noexcept;

  // Move constructor 
  thread(thread&& Other) noexcept;
#+END_SRC

 + _Other Constructors_
   + The following constructor can take as agument (type parameter
     Fn): any function pointer; callable object (aka "functor") or
     lambda expression. The thread of execution associated to the
     constructed object starts its execution immediately. 

#+BEGIN_SRC cpp 
  template <class Fn, class... Args>
  explicit thread(Fn&& F, Args&&... A);
#+END_SRC

Example: Construct thread out of function pointer: 

#+BEGIN_SRC cpp 
  //========= Create thread out of function pointer =======// 
  void do_forever() 
  { 
    while(every 10 seconds){ println(" 10 second elapsed!")};
  }  

  void action_sleep(int N)
  { 
     // .... sleep for N seconds ....
     print(" [INFO] Thread wake up! OK");
  }

  thread th1 {do_forever};
  thread th2 {&do_forever};
  thread th3 {action_sleep, 10}; 
  thread th4 {&action_sleep, 1};   
#+END_SRC

Example: construct threads out of function object, aka callable
objects or functors. 

#+BEGIN_SRC cpp 
  // Functor
  struct LoopMessage
  {
      const std::string message;
      const int delay;

      LoopMessage(std::string message, int delay):
          message(message)
        , delay(delay) { }

      // Function-call operator called by the thread class. 
      void operator()()
      {
          while(true)
          {
              std::this_thread::sleep_for(std::chrono::seconds(delay));
              std::cout << " [INFO] thread id = " << std::this_thread::get_id()
                        << " ; " << message << std::endl;
          }
      }
  };

  int main()
  {
      std::thread thread_messageA {LoopMessage, "Hello world", 10};
      auto thread_messageB = std::thread{LoopMessage, "Hello world", 10}; 
      return 0;
  }
#+END_SRC

Example: construct thread object out of lambda expressions. 

#+BEGIN_SRC cpp 
    std::thread threadA ([](){
        while(true)
        {
            std::this_thread::sleep_for(std::chrono::seconds(1));
           // ... action .... // 
        }
    });

    auto th4 = std::thread([](int N){
        while(true)
        {
            std::this_thread::sleep_for(std::chrono::seconds(1));
            // .... action ... // 
        }
    }, 10);
#+END_SRC

Note: Before a given std::thread object goes out of scope, it is
necessary to call the methods .join() for waiting for the completion
of associated thread of execution or std::thread::detach() for
detaching the thread of execution. If neither of those functions are
called, the C++ runtime calls _std::terminate_ and _std::abort()_
indirectly which causes abnormal termination of the current
application.  

Example: 

#+BEGIN_SRC cpp 
  // Failure => Te runtime will call std::terminate 
  int Function_error()
  {
     std::thread threadA {&functionPointer, 10, "Hello world"};
     // Error: Missing call to methods .detach() or .join() 
     // The runtime will call std::terminate() causing abnormal terminatoon!!
     return 1;
  }

  // OK => Does not call std::terminate. 
  int Function_join()
  {
     std::thread threadA {&functionPointer, 10, "Hello world"};
     // Ok 
     threadA.join(); // Wait for completion of threadA. 
     return 1;
  } // ThreaA out of scope here! 

  // OK => Does not call std::terminate.
  int Function_detach()
  {
     std::thread threadA {&functionPointer, 10, "Hello world"};
     // Ok 
     // Detach, no longer control or manage threadA. 
     threadA.detach(); 
     return 1;
  }

  int main() 
  { 
     Function_error(); 
     Function_join(); 
     Function_detach();

     return 10; 
  }
#+END_SRC


 + _joinable()_
   + => Returns true if the thread is _joinable_, in other words, if the
     thread of execution associated to the called object is running.  

#+BEGIN_SRC cpp 
  bool joinable() const noexcept;
#+END_SRC

 + _join()_
   + => Blocks the current thread waiting for the completion of the
     called object thread. For instance, calling threadA.join() will
     block the current thread waiting until the thread of execution of
     threadA object finishes. 

#+BEGIN_SRC cpp 
  void thread::join();
#+END_SRC

 + _detach()_ 
   + => Detaches the associated thread of execution from a given
     std::thread object. After this function is called, it is no
     longer possible to control the detached thread of execution or
     joining it (waiting for its completion). Then, the std::thread
     object no longer represents the detached execution thread. 
   + => A thread of execution that was detached is also called _daemon thread_. 

#+BEGIN_SRC cpp 
   void thread::detach();
#+END_SRC

 + _id()_
   + => Returns the unique ID indentifier number for each thread. 

#+BEGIN_SRC cpp 
  int thread::id () noexcept;
#+END_SRC

 + _hardware_concurrency()_
   + => Returns an estimate for the number of threads that can be run
     in parallel. The result is often equal to the number of _logical CPU cores_.

#+BEGIN_SRC cpp 
  static unsigned int thread::hardware_concurrency() noexcept;
#+END_SRC

** Functions of namespace std::this_thread 

Utilities functions for currrent thread of execution: 

Header: 
  + [[https://en.cppreference.com/w/cpp/thread][<header>]]

Function Documentation: 

  + [[https://en.cppreference.com/w/cpp/thread/yield][this_thread::yield]]

  + [[https://en.cppreference.com/w/cpp/thread/get_id][this_thread::get_id]]

  + [[https://en.cppreference.com/w/cpp/thread/sleep_for][this_thread::sleep_for]]

  + [[https://en.cppreference.com/w/cpp/thread/sleep_until][this_thread::sleep_until]] 

Signature of functions in namespace this_thread. 

#+BEGIN_SRC cpp 
      // Namespace: std::this_thread. 
      namespace std { namespace this_thread {
          // Returns the id of the current thread.
          std::thread::id get_id() noexcept;
          // Provides a hint to the implementation to reschedule the
          // execution of threads, allowing other threads to run.
          void            yield() noexcept;      
          // Blocks the execution of the current thread for at least the specified sleep_duration.
          template< class Rep, class Period >
          void            sleep_for( const std::chrono::duration<Rep, Period>& sleep_duration );
          // Blocks the execution of the current thread until specified
          // sleep_time has been reached.
          template< class Clock, class Duration >
          void           sleep_until( const std::chrono::time_point<Clock,Duration>& sleep_time );
       }
      }
#+END_SRC


** Thread - std::thread usage and synchronization primtives 
*** Race condition 

When the a race condition happens, the outcome of the computation with
a shared resource depends precisely on the order of execution of the
threads. Race condition bugs are hard to trace and debug. The solution
to this flaw is to coordinate the thread access to shared resources
through synchronization primitives, namely, mutex (mutual exclusion
locks), atomic variables, and so on.

Most common types of shared resources: 
  + Global variable or objects such as: std::cout, std::cerr, std::cin
  + Shared variables between threads
  + Singleton objects - class with an unique global instance. 

C++ Standard definition about *Data Race* (aka race condition): 

#+BEGIN_QUOTE
  The execution of a program contains a *data race* if it conains two
  potential concurrent conflicting actions, at least one of which is
  not atomic, and neither happens before the other, except of the
  special case for singnal handlers described below. *Any such data*
  *race results* in *undefined behavior*.
#+END_QUOTE

C++ Standard about Undefined Behavior: 

#+BEGIN_QUOTE
  A conforming implementation executing a well-formaed program shall
  reproduce the same observable behavior as one of the possible
  executions of the corresponding instances of the abstract machien
  with the smae program and the same input. However, if any such
  exeuction contains an undefined operation, this International
  Standard places no requirement on the implementation executing that
  program with that input.
#+END_QUOTE

  *Example about race condition:*

Race condition: 

 + File: race_condtion.cpp

#+BEGIN_SRC cpp 
  #include <iostream>
  #include <thread>
  #include <chrono>
  #include <thread>
  #include <vector>

  struct Worker
  {
      int& acc;

      Worker(int& acc): acc(acc) { }

      void operator()(int x)
      {
          std::this_thread::sleep_for(std::chrono::milliseconds(500));
          acc = acc + x * x;
      }
  };

  int main()
  {
      int result = 0;

      std::vector<std::thread> thread_list{};
      for(int i = 1; i <= 10; i++)
      {
          thread_list.push_back( std::thread{Worker(result), i} );
      }

      for(auto& t: thread_list) { t.join(); }
      std::cout << " result   = " << result << std::endl;
      return 0;
  }
#+END_SRC

Building: 

#+BEGIN_SRC sh 
  $ g++ thread1.cpp -o thread1.bin -std=c++1z -Wall -Wextra -O0 -g -lpthread 
#+END_SRC

Running: 
  + The expected result is 385. However, the program sometimes yields
    an incorrect result due to a _race condition_ bug (aka data
    race). 

#+BEGIN_SRC sh 
  $ ./thread1.bin 
   result   = 385

  $ ./thread1.bin 
   result   = 384

  $ ./thread1.bin 
   result   = 385

  $ ./thread1.bin 
   result   = 368

  $ ./thread1.bin 
   result   = 385

  ./thread1.bin 
   result   = 376
#+END_SRC
*** Mutex solution 

The race condition can be solved by using *mutex* - mutual exclusion
synchronization primitive which allows only a single thread at a time
to access the critical section, portion of the code with a shared
resource.   

File: thread2.cpp 

#+BEGIN_SRC cpp 
  #include <iostream>
  #include <thread>
  #include <chrono>
  #include <thread>
  #include <vector>
  #include <mutex>

  using namespace std::chrono_literals;

  struct Worker
  {
      int& acc;
      // Requires <mutex> header
      std::mutex& m;

      Worker(int& acc, std::mutex& m): acc(acc), m(m) { }

      void operator()(int x)
      {
          std::this_thread::sleep_for(std::chrono::milliseconds(500));
          // --- Start of critical section ---- //
          m.lock();   // Curren thread acquire locks
          acc = acc + x * x;
          m.unlock(); // Current thread releases lock
          // --- End of critical section ---- //
      }
  };


  int main()
  {
      // Shared resource
      int result = 0;
      std::mutex m;

      std::vector<std::thread> thread_list{};
      for(int i = 1; i <= 10; i++)
      {
          thread_list.push_back( std::thread{Worker(result, m), i} );
      }

      for(auto& t: thread_list) { t.join(); }

      std::cout << " result   = " << result << std::endl;

      return 0;
  }
#+END_SRC

Building: 

#+BEGIN_SRC sh 
  $ g++ thread2.cpp -o thread2.bin -std=c++1z -Wall -Wextra -O0 -g -lpthread 
#+END_SRC

Running:  

  + The computation becomes reproducible and predictable due to the
    mutex allow only a single thread at atime access the shared
    resource (variable result). 

#+BEGIN_SRC sh 
  $ ./thread2.bin 
   result   = 385

  $ ./thread2.bin 
   result   = 385

  $ ./thread2.bin 
   result   = 385

  $ ./thread2.bin 
   result   = 385


  $ ./thread2.bin 
   result   = 385
#+END_SRC

Note: The current code is not exception safe and error prone, as a
result if an exception happens or if the lock releasing code is
missing, the outcome will be a _deadlock._ It is better to use the
std::mutex_guard which is an RAII (Resource Acquisition Is
Initialization) wrapper for locks. When the mutex guard object is
constructed, the current thread acquires the lock and when the guard
goes out of scope, the mutex lock is released. So, by using a
_scope_guard_, the code becomes:

#+BEGIN_SRC cpp 
    void operator()(int x)
    {
        std::this_thread::sleep_for(std::chrono::milliseconds(500));
        // --- Start of critical section ---- //
        // Acquires lock 
        std::lock_guard<std::mutex> mutex_guard(m);
        acc = acc + x * x;
        // --- End of critical section ---- //
    } // Releases lock here, when the mutex_guard goes out of scope and is destroyed. 
#+END_SRC

*** Atomic variable solution 

Another way to solve the race condition (aka data race) problem is
using _atomic variables and atomic operations_. 

File: thread3.cpp 

#+BEGIN_SRC cpp 
  #include <iostream>
  #include <thread>
  #include <chrono>
  #include <thread>
  #include <vector>
  #include <mutex>
  #include <atomic>

  using namespace std::chrono_literals;

  struct Worker
  {
      std::atomic<int>& acc;
      // Requires <mutex> header
      std::mutex& m;

      Worker(std::atomic<int>& acc, std::mutex& m)
         : acc(acc), m(m) { }

      void operator()(int x)
      {
          std::this_thread::sleep_for(std::chrono::milliseconds(500));
          acc += x * x;
      }
  };

  int main()
  {
      // Shared resource
      std::atomic<int> result = 0;
      std::mutex m;

      std::vector<std::thread> thread_list{};
      for(int i = 1; i <= 10; i++)
      {
          thread_list.push_back( std::thread{Worker(result, m), i} );
      }

      for(auto& t: thread_list) { t.join(); }

      std::cout << " result   = " << result << std::endl;

      return 0;
  }
#+END_SRC

Output: 

#+BEGIN_SRC sh 
  $ ./thread3.bin 
   result   = 385

  $ ./thread3.bin 
   result   = 385

  $ ./thread3.bin 
   result   = 385

  $ ./thread3.bin 
   result   = 385
#+END_SRC


** Thread - Returning values from std::thread 
*** Return value from thread 

There is no way to return a value from a std::thread computaiton, if a
function used for instantiating std::thread has any returning value it
is ignored.  

Example: 

#+BEGIN_SRC cpp 
   double heavy_computation(doube input)
   {
       // do something .... heavy calculation 
       return output; 
   }

  // Return value ignored. 
  auto th = std::thread( &heavy_computation, 10);

 // ... ... ... 

  th.join();
#+END_SRC

The workaround for returning a value out of std::thread is to set a
variable defined outside of the thread or set a parameter passed as
pointer or reference.

 + Return value from thread by setting a variable defined outside of
   the thread. 

#+BEGIN_SRC cpp 
  double output = 0.0;

  // Return value ignored. 
  auto th = std::thread( &[&]
           { 
              // perform heavy computation // 
               .... 
              /* set output */
              output =  .... 
           }, 10);

 // ... ... ... 

  th.join();
  printf(" output = %f", output);
#+END_SRC


 + Return value from thread by setting the function parameters passed
   by reference or pointer. 

#+BEGIN_SRC cpp 
   void heavy_computation(doube input, double& output1, double* output2)
   {
       // do something .... heavy calculation 
       output1 =  .... ;
       *output2 = "Something else ...."; 
   }

   double      output1 = 0.0; 
   std::string output2 = "";
 
   auto th = std::thread(&heavy_computation, std::ref(output1), &output2);
   th.join();
 
   std::cout << " output1 = " << output1 " ; " << " output2 = " << output2 << "\n";
#+END_SRC
*** Catch exceptions in threads 

If any exception is thrown in a std::thread, the exception is not
propagated to the try ... catch block outside of the thread, instead
the C++ runtime calls std::terminate causing abonormal termination. 

 *Example: code does not work => calls std::terminate*

#+BEGIN_SRC cpp 
  auto do_something = [](int input)
  {
     // ....  .... // 
     if(input < 0 ){ throw std::logic_error("Invalid input"); }
     //.... ... ... ... //
  };

  // DOES NOT WORK =>>> C++ runtime calls std::terminate !!!
  try
  {
     auto th = std::thread(do_something, -10);
  } catch(std::logic_error& ex)
  {
     std::cout << " Error: " << ex.what() << "\n".;
  }
#+END_SRC

 *Example: Solution* 

#+BEGIN_SRC cpp 
  std::exception_ptr exptr = nullptr;

  auto do_something = [&](int input)
  {
    try {
     // ....  .... // 
     if(input < 0 ){ throw std::logic_error("Invalid input"); }
     //.... ... ... ... //

    // Catch all exceptions 
    } catch(...) { 
        exception_ptr = std::current_exception();
    }
  };

  try
  {
     auto th = std::thread(do_something, -10);
     th.join(); 
     std::rethrow_exception(exptr);
  } catch(std::logic_error& ex)
  {
     std::cout << " Error: " << ex.what() << "\n".;
  }
#+END_SRC
*** Example: Returning values and catching exceptions 

File: return-thread.cpp 

#+BEGIN_SRC cpp 
  // Experiment returning results from threads
  //---------------------------------------------
  #include <iostream>
  #include <chrono>
  #include <thread>
  #include <vector>
  #include <functional>

  using namespace std::chrono_literals;
  namespace cr = std::chrono;

  struct Timer
  {
      decltype(cr::steady_clock::now()) start;

      Timer(){  start = cr::steady_clock::now();  }

      ~Timer()
      {
          auto end = cr::steady_clock::now();
          auto duration = cr::duration_cast<cr::seconds>(end - start);
          std::cout << " [TRACE] Elapsed time: " << duration.count()
                    << std::endl;
      }
  };


  double heavy_computation(int input)
  {
      using namespace std::string_literals;

      std::this_thread::sleep_for(4s);
      if(input < 0) {
          throw std::runtime_error("Error: invalid input: "s + std::to_string(input));
      }
      return input * 3.815 + 5.6;
  }


  int main(int argc, char* argv[])
  {
      std::vector<int>         inputs{2, 10, 5};
      std::vector<double>      results(3);
      std::vector<std::thread> threads;
      threads.reserve(3);

      // Note: A std::thread will never return anything from a function
      // passed as argument, even if has non-void return type.
      // The only to return a value from a thread is to set an
      // argument passed by pointer or reference or set a global object.
      auto thread_adapter = [&](size_t index, int input)
      {
          results[index] = heavy_computation(input);
      };

      std::puts("\n ===== Experiment 1 - Return value from threads (Parallel Computing) ==== \n");

      {   // Shows elapsed time at end of this scope
          auto timer = Timer{};

          for(size_t i = 0; i < 3; i++) {
              threads.push_back( std::thread(thread_adapter, i, inputs[i]) );
          }

          std::puts(" [INFO] Waiting for thread completion");

          // Wait for the completion of all threads
          for(size_t i = 0; i < 3; i++) { threads[i].join(); }

          // Show results
          for(size_t i = 0; i < 3; i++) {
              std::printf(" Result[%lu] = %f\n", i, results[i]);
          }
      }

      ///---------------------------------------------------------------//
      std::puts("\n ===== Dealing with exceptions from threads ==== \n");

      double output = 0.0;
      std::exception_ptr exptr = nullptr;

      auto thread_adapter2 = [&](int input)
      {
          std::puts(" [TRACE] Inside thread_adapter2");

          // Note: if an exception is not caught,
          // it will not be propagated to the parent thread.
          // Instead, the C++ runtime will call std::terminate
          // causing abonormal temrination.
          try
          {
              output = heavy_computation(input);
          } catch (...)
          {
              // Catch all exceptions from the computation that
              // should run in a new thread and set a shared memory variable
              // set only once by this thread.
              exptr = std::current_exception();
          }
      };

      auto th = std::thread(thread_adapter2, -10);
      std::puts(" [TRACE] Waiting thread_adapter2 thread termination.");
      th.join();

      if(!exptr)
      {
          std::cout << " [INFO] Result of thread_adapter2 is: " << output << "\n";

      } else
      {
          try
          {
              std::rethrow_exception(exptr);
          } catch (std::runtime_error& err) {
              std::cout << " [ERROR] " << err.what() << "\n";
          }
      }

      return 0;
  }

#+END_SRC

Building: 

#+BEGIN_SRC sh 
  $ g++ return-thread.cpp -o out.bin -std=c++1z -O0 -g -lpthread
#+END_SRC

Running: 

#+BEGIN_SRC sh 
  $ ./out.bin 

   ===== Experiment 1 - Return value from threads (Parallel Computing) ==== 

   [INFO] Waiting for thread completion
   Result[0] = 13.230000
   Result[1] = 43.750000
   Result[2] = 24.675000
   [TRACE] Elapsed time: 4

   ===== Dealing with exceptions from threads ==== 

   [TRACE] Waiting thread_adapter2 thread termination.
   [TRACE] Inside thread_adapter2
   [ERROR] Error: invalid input: -10

#+END_SRC

** Task-based APIs => Futures and Promises 
*** Futures overview 

Overview: 

  + The std::future class encapsulates a value that will eventually
    become available from an asynchronous computation. 

  + This class makes easier to run functions or anything callable that
    returns value in a new thread and get this value without any
    global variables. Another benefit is that is much easier to handle
    exceptions from another thread than the class std::thread.

  + Use cases:
    + Short computations that returns a value such as a network request.
    + Implement parallel algorithms and parallel computations. 

  + Not the best use case:
    + Long running thread or worker thread.

  + Problems:
    + Many other implementations of futures run the tasks in a
      thread-pool, a pre-allocated set of threads of spawning too much
      threads that would increase the memory footprint, reducing the
      performance and throughput.
    + The C++ standard library's std::future API does not use thread
      pools.

 *Documentation*

  + [[https://en.cppreference.com/w/cpp/thread/promise][std::promise]]

  + [[https://en.cppreference.com/w/cpp/atomic/memory_order][std::memory_order]]

 *Other Implementations of Futures and Promises*

  + [[https://www.boost.org/doc/libs/1_71_0/doc/html/thread/synchronization.html#thread.synchronization.futures][boost::future]]

  + [[https://github.com/facebook/folly/blob/master/folly/docs/Futures.md][folly::Future]] 

  + [[http://stlab.cc/libraries/concurrency/future/future/][stlab::future]] 

  + hpx::future
*** Usage 

The function [[https://en.cppreference.com/w/cpp/thread/async][std::async]] is used for creating  [[https://en.cppreference.com/w/cpp/thread/future][std::future]] objects out of
functions that return values.

#+BEGIN_SRC cpp 
  // Note: simplified pseudo-signature => 
 
  // Overload 1: 
  template<typename Callable, typename ... Args>
  auto async( Function&& f, Args&&... args ) -> std::future<Return>;

  // Overload 2: 
  template<typename Return, typename Callable, typename ... Args>
  auto async( std::launch policy, Function&& f, Args&&... args ) -> std::future<Return>;
#+END_SRC

Usage example: 

  + The object futA represents the return value or the result of the
    computation heavy_computation that is run in a new thread not
    blocking the thread that called std::async.

  + The parameter _std::launch::async_ => or launch policy is necessary
    to run the computation in a new thread. 

#+BEGIN_SRC cpp 
   double heavy_computation(int size, std::string label, bool flag)
   { 
     // Sleep blocking the current thread for 10 seconds.
     std::this_thread::sleep_for(10s);
     // ... ... ... .. 
     return output; 
  }

   std::future<double> futA = std::async(std::launch::async, heavy_computation, 100, "calc1", false);

   auto futB = std::async(std::launch::async, [](int n)
              { 
                 // .... heavy CPU bound computation ... // 
                 double output = ...; 
                   ... ... ... 
                 return output;
              }, 200);
#+END_SRC

  + _std::future<T>::get()_
    + The member function .get() from [[https://en.cppreference.com/w/cpp/thread/future][std::future]] blocks the current
      thread (similar to std::thread::join) waiting for the completion
      of the computation and then returns its result.

#+BEGIN_SRC cpp 
    // Block current thread - similar to std::thread::join()
    double resultA = futA.get(); 
    double resultB = futB.get(); 

    std::cout << " resultA = " << resultA << " ; " << resultB << "\n";
#+END_SRC

  + _std::fuuture<T>::wait()_
    + The member function .wait() blocks the current thread waiting the
      std::future's thread termination.  

#+BEGIN_SRC cpp
  // Wait for the computation completion blocking current thread.
  futB.wait(); 
#+END_SRC
 
  + Catching exceptions.

#+BEGIN_SRC cpp 
    double computation(int input)
    { 
      ... .... ... ... 

      if(bad_input(input)) { 
          throw std::runtime_error("Invalid input, try again ..."); 
      }

      ... .... ... ... 
      return result; 
   }

    // Type: std::future<double>
    auto afuture = std::async(std::launch::async, computation, 10);

    try
    { 
      double result = afuture.get(); 
      std::cout << " Result = " << result << "\n";
    } catch(std::runtime_error& ex)
    {
      std::cout << " [ERROR] " << ex.what() << "\n":
    }
#+END_SRC

 + Run computations in parallel 

#+BEGIN_SRC cpp 
    double computation(int input)
    { 
      ... .... ... ... 
      // Sleep blocking this thread for 8 seconds 
      // for simulating a heavy CPU-bound computation.
      std::this_thread::sleep_for(8s);
      ... .... ... ... 
      return result; 
   }

   std::vector<std::future<double>> futures; 
   futures.reserver(3);
   std::vector<int> inputs {10, 20, 25};
   std::vector<double> outputs; 
     
  for(auto& x: inputs){ 
     futures.push_back( std::async(std::launch::async, computation, x);   ) 
  };

  // Block current thread waiting all computations finish. 
  // If every computation takes 8 seconds, the total time waiting 
  // will be 8 seconds since they are all run in parallel. 
  for(auto& fut: futures)
  {
     double x = fut.get(); 
     outputs.push_back(x);
     std::cout << "Result = " << x << "\n";
  }
#+END_SRC
*** Example 

File: future1.cpp 

#+BEGIN_SRC cpp 
  #include <iostream>
  #include <thread>
  #include <chrono>

  #include <queue>
  #include <vector>

  // --- Concurrency Headers ---- //
  #include <thread>
  #include <mutex>
  #include <future>
  #include <type_traits>
  #include <optional>

  namespace cr = std::chrono;
  using namespace std::chrono_literals;

  #define LOG_FUNCTION_ENTRY() \
            std::cerr << " [TRACE-ENTRY] Function = " << __FUNCTION__  \
            << " ; threadID " << std::this_thread::get_id() << std::endl


  struct TimeCounter
  {


      decltype(std::chrono::steady_clock::now()) start;
      TimeCounter(){  start = cr::steady_clock::now();  }

      void report(std::string const& label)
      {
          auto end = cr::steady_clock::now();
          auto duration = cr::duration_cast<cr::seconds>(end - start);
          std::cout << " [TRACE] Elapsed time: {" << label << "} = " << duration.count()
                    << std::endl;
      }
  };

  std::mutex mu;

  double expensive_computationA(int n)
  {
      {   /* --- Start of critical section --- */
          // Protect std::cout from race condition
          std::lock_guard<std::mutex> guard(mu);
          LOG_FUNCTION_ENTRY();
      }   /* --- End of critical section --- */
      std::this_thread::sleep_for(4s);
      return 4.0 * n + 5;
  }

  double expensive_computationB(int n)
  {
      {   /* --- Start of critical section --- */
          // Protect std::cout from race condition
          std::lock_guard<std::mutex> guard(mu);
          LOG_FUNCTION_ENTRY();
      }   /* --- End of critical section --- */
      std::this_thread::sleep_for(8s);
      return 2.515 * n + 15.8714;
  }

  std::string
  computatio_with_exception(int n)
  {
      LOG_FUNCTION_ENTRY();
      std::this_thread::sleep_for(1s);
      if(n < 0) { throw std::logic_error(" Error: not allowed negative N"); }
      return "Hello world";
  }



  int main(int argc, char** argv)
  {

      std::cout << " Number of hardware threads = "
                << std::thread::hardware_concurrency() << std::endl;

  #if 1

      LOG_FUNCTION_ENTRY();

      std::puts("\n ==== EXPERIMENT 1 >>  without std::future => Run Serially ==== ");
      std::puts("-----------------------------------------------------------------\n");
      {
          TimeCounter tc;
          double resultA = expensive_computationA(10);
          double resultB = expensive_computationB(10);

          std::cout << " [TRACE] Completed =>> resultA = " << resultA
                    << " ; resultB = " << resultB
                    << std::endl;
          tc.report("EXPERIMENT1");
      }

      std::puts("\n ==== EXPERIMENT 2 >> with std::futures - Run in Parallel ===== ");
      std::puts("-----------------------------------------------------------------\n");
      {
          TimeCounter tc;


          // Function runs in another thread when the future object
          // is instantiated.
          // The default policy of std::async is (async | deferred) which
          // would let the C++ runtime decide when run the thread.
          std::future<double> futA = std::async(std::launch::async
                                                , &expensive_computationA, 10);

          // Function runs only when the std::future<T>::get() method is called.
          // The function expensive_computationB is not run at the moment futB object
          // is created.
          auto futB = std::async(std::launch::async, expensive_computationB, 10);

          // Protect std::cout from race condition (aka data race)
          {
              std::lock_guard<std::mutex> guard(mu);
              std::cout << " [TRACE] Waiting futures results " << std::endl;
          }

          // Calls to .get() method blocks the current thread and waits
          // for the completion of the computations wrapped in the future object.
          double resultB = futB.get();
          double resultA = futA.get();

          std::cout << " [TRACE] Completed =>> resultA = " << resultA
                    << " ; resultB = " << resultB
                    << std::endl;

          tc.report("EXPERIMENT2");

          // The method can only be used only once!!
          // otherwise, it calls std::termiante causing abnormal termination.
          //---------------------------------
          // double resultAA = futA.get(); // DO NOT!
      }

      std::puts("\n ==== EXPERIMENT 3 >> Exceptions handling ====================== ");
      std::puts("-----------------------------------------------------------------\n");
      {
          std::cout << "Future Created" << "\n";
          auto fut1 = std::async(computatio_with_exception, -10);

          try {
              auto result1 = fut1.get();
              std::cout << " [INFO] Result1 = " << result1 << "\n";
          } catch(std::logic_error& ex)
          {
              std::cerr << " [ERROR] " << ex.what() << "\n";
          }

      }

  #endif

      return 0;
  }
#+END_SRC

Building: 

#+BEGIN_SRC sh 
   clang++ future1.cpp -o future1.bin -std=c++1z -lpthread -Wall -Wextra -O0 -g
#+END_SRC

Output: 

#+BEGIN_SRC sh 
   $ ./future1.bin 

   Number of hardware threads = 4

   ==== EXPERIMENT 1 >>  without std::future => Run Serially ==== 
  -----------------------------------------------------------------

   [TRACE-ENTRY] Function = main ; threadID 139662702782272
   [TRACE-ENTRY] Function = expensive_computationA ; threadID 139662702782272
   [TRACE-ENTRY] Function = expensive_computationB ; threadID 139662702782272
   [TRACE] Completed =>> resultA = 45 ; resultB = 41.0214
   [TRACE] Elapsed time: {EXPERIMENT1} = 12

   ==== EXPERIMENT 2 >> with std::futures - Run in Parallel ===== 
  -----------------------------------------------------------------

   [TRACE] Waiting futures results 
   [TRACE-ENTRY] Function = expensive_computationA ; threadID 139662684923648
   [TRACE-ENTRY] Function = expensive_computationB ; threadID 139662676530944
   [TRACE] Completed =>> resultA = 45 ; resultB = 41.0214
   [TRACE] Elapsed time: {EXPERIMENT2} = 8

   ==== EXPERIMENT 3 >> Exceptions handling ====================== 
  -----------------------------------------------------------------

  Future Created
   [TRACE-ENTRY] Function = computatio_with_exception ; threadID 139662684923648
   [ERROR]  Error: not allowed negative N
#+END_SRC
** Condition Variables and Producer Consumer Problem 

  + A condition variable is a synchronization primitive which allows
    one or more threads to wait for a event, signal from another
    thread, without wasting CPU cycles. 

  + Mechanism: several threads wait on a condition variable, until
    another thread notifies this synchronization primitive.

  + Note: Condition variables do not provide locking such as Mutexes,
    so they must be used alongside condition variables in order to
    avoid race conditions.

  + Operations of condition Variables:

    + ConditionVar.wait(MutexLock)
      + => Makes current thread (waiting thread) sleep waiting from a signal

    + ConditionVar.notify_one()
      + => Wakes up a waiting thread. If there is no waiting thread,
        the operation does nothing.

    + ConditionVar.notify_all()
      + => Wake up all waiting threads.

 *Condition Variable Method Signatures*

#+BEGIN_SRC cpp 
  class condition_variable
  {
   public:
      condition_variable();
      ~condition_variable();
      condition_variable(const condition_variable&) = delete;
      condition_variable& operator=(const condition_variable&) = delete;

      /** Wake up only one sleeping thread */
      void notify_one() noexcept;
    
      /** Wake up all sleeping threads that are waiting for this signal */      
      void notify_all() noexcept;

      /** Make thread which alls this method sleep (wait for signal)
        ,* without waste CPU cycles */  
      void wait(unique_lock<mutex>& lock);

      template <class Predicate>
          void wait(unique_lock<mutex>& lock, Predicate pred);

      template <class Clock, class Duration>
      cv_status wait_until(unique_lock<mutex>& lock, const chrono::time_point<Clock, Duration>& abs_time);

      template <class Clock, class Duration, class Predicate>
      bool wait_until(unique_lock<mutex>& lock, const chrono::time_point<Clock, Duration>& abs_time, Predicate pred);

      template <class Rep, class Period>
      cv_status wait_for(unique_lock<mutex>& lock, const chrono::duration<Rep, Period>& rel_time);

      template <class Rep, class Period, class Predicate>
      bool wait_for(unique_lock<mutex>& lock, const chrono::duration<Rep, Period>& rel_time, Predicate pred);

      typedef implementation-defined native_handle_type;
      native_handle_type native_handle();
  };
#+END_SRC

 *Example: Usage of Conditions Variables in Producer/Consumer problem* 

The producer consumer problem is classical synchronization problem
where a producer puts data into a data structure and another thread, called
consumer, removes data from the data structure. Only a single thread
should be able to access the data structure at any atime. 

Sample implementation: https://gist.github.com/iikuy/8115191

Example: 
 
  + File: producer-consumer.cpp

#+BEGIN_SRC cpp 
  #include <iostream>
  #include <thread>
  #include <chrono>

  #include <queue>
  #include <vector>

  // --- Concurrency Headers ---- //
  #include <thread> // threads
  #include <mutex>  // mutex, lock_guard, unique_lock
  #include <future> // conditional_variables

  using namespace std::chrono_literals;

  int main(int argc, char** argv)
  {

      std::cout << " Number of hardware threads = "
                << std::thread::hardware_concurrency() << "\n\n";

      std::condition_variable cond;
      std::mutex m;
      std::queue<double> buffer;
      bool finished = false;

      auto producer_thread = std::thread([&]
      {
              std::cout << " [PRODUCER] Producer thread started." << "\n";
              for(int i = 0; i < 5; i++)
              {
                  std::this_thread::sleep_for(1s);
                  {  // -- start of critical section ----//
                      auto lock = std::lock_guard<std::mutex>{m};
                      double x = 5 * i + 10;
                      std::cout << "\n [PRODUCER] Send data to buffer x = " << x << "\n";
                      buffer.push(x); // Mutex protects the buffer fron race condition
                    // --- End of ciritical section ---- //
                  }

                  // Send signal notifying consumer thread to proceed.
                  cond.notify_all();
              }

              {
                  auto lock = std::lock_guard<std::mutex>{m};
                  std::cout << " [PRODUCER] End of transmission" << std::endl;
                  finished = true;
              }
      });

      auto consumer_thread = std::thread([&]{
          std::cout << " [CONSUMER] Consumer thread started." << "\n";
          while(true)
          {            
              auto lock = std::unique_lock<std::mutex>{m};

              std::cout << " [CONSUMER] Waiting input " << "\n";

              // The condition variable waits for cond.notify_one() signal
              // from the producer thread. Before this signal is sent, this
              // thread sleeps until receives it.
              cond.wait(lock, [&]{ return !buffer.empty(); });

              std::cout << " [CONSUMER] Processing data ... wait" << "\n";
              // Delay for simulating processing time
              std::this_thread::sleep_for(5s);

              std::cout << " [CONSUMER] Received value " << buffer.front() << "\n";
              buffer.pop();

              if(finished){
                  std::cout << " [CONSUMER] Stop consumer thread. Ok" << "\n";
                  break;
              }

              // Unlock in order to avoid deadlock
              // lock.unlock();
          }
      });

      std::cout << " [TRACE] Waiting thread completion" << "\n";
      producer_thread.join();
      consumer_thread.join();

      return 0;
  }
#+END_SRC

Building and Running: 

#+BEGIN_SRC sh 
  $ g++ producer-consumer.cpp -o out.bin -std=c++1z -O0 -g -Wall -lpthread
#+END_SRC

Running: 

#+BEGIN_SRC sh 
  $ ./out.bin 
   Number of hardware threads = 4

   [PRODUCER] Producer thread started.
   [TRACE] Waiting thread completion
   [CONSUMER] Consumer thread started.
   [CONSUMER] Waiting input 

   [PRODUCER] Send data to buffer x = 10
   [CONSUMER] Processing data ... wait
   [CONSUMER] Received value 10
   [CONSUMER] Waiting input 

   [PRODUCER] Send data to buffer x = 15
   [CONSUMER] Processing data ... wait
   [CONSUMER] Received value 15
   [CONSUMER] Waiting input 

   [PRODUCER] Send data to buffer x = 20
   [CONSUMER] Processing data ... wait
   [CONSUMER] Received value 20
   [CONSUMER] Waiting input 

   [PRODUCER] Send data to buffer x = 25
   [CONSUMER] Processing data ... wait
   [CONSUMER] Received value 25
   [CONSUMER] Waiting input 

   [PRODUCER] Send data to buffer x = 30
   [PRODUCER] End of transmission
   [CONSUMER] Processing data ... wait
   [CONSUMER] Received value 30
   [CONSUMER] Stop consumer thread. Ok

#+END_SRC


** Producer/consumer problem with concurrent blocking queue 

The producer consumer synchronization problem can be simplified with a
concurrent blocking queue which encapsulates the data structure
implementation and the all synchronization locks. 

Requirements: 

  * The _producer thread_ puts an element in the queue and signals the
    condition variable what wakes up the _consumer thread_. 

  * When the queue is empty, the _consumer thread_ gets in sleep state
    and waits for some element to be put into the queue. 

  * The access to the queue must be thread-safe. 

Parts of standard library used: 

  + [[https://en.cppreference.com/w/cpp/thread/thread][std::thread]]
  + [[https://en.cppreference.com/w/cpp/thread/condition_variable][std::condition_variable]]
  + [[https://en.cppreference.com/w/cpp/thread/mutex][std::mutex]]
  + [[https://en.cppreference.com/w/cpp/thread/unique_lock][std::unique_lock]]
  + [[https://en.cppreference.com/w/cpp/thread/lock_guard][lock:guard]]
  + [[https://en.cppreference.com/w/cpp/container/queue][queue]] 

 *Example:* Simple Blocking Queue Implementation for consumer/producer problem. 

File: concurrent_queue.cpp 

#+BEGIN_SRC cpp 
  #include <iostream>
  #include <thread>
  #include <chrono>

  #include <queue>
  #include <vector>
  #include <functional>

  // --- Concurrency Headers ---- //
  #include <thread> // threads
  #include <mutex>  // mutex, lock_guard, unique_lock
  #include <future> // conditional_variables

  using namespace std::chrono_literals;

  template<typename T>
  class ConcurrentQueue
  {
      using guard = std::lock_guard<std::mutex>;

      std::queue<T>           m_queue;
      std::condition_variable m_cvar;
      mutable std::mutex      m_mutex;

  public:
      ConcurrentQueue(){}

      // Called by producer thread
      void push(T& value)
      {
          auto g = guard(m_mutex);
          m_queue.push(value);
          m_cvar.notify_one();
      }

      void push(T&& value)
      {
          auto g = guard(m_mutex);
          m_queue.push(value);
          m_cvar.notify_one();
      }


      // Called by consumer thread.
      // blocks consumer thread making
      // it sleep when the queue is empty
      void pop()
      {
          auto g = guard(m_mutex);
          m_queue.pop();
      }

      // Called by consumer thread
      T& front()
      {
          auto lock = std::unique_lock<std::mutex>(m_mutex);
          // The second argument of m_cvar.wait, a lambda function is
          // passed as predicate to prevent spurious thread wake up.
          m_cvar.wait(lock, [&]{ return !m_queue.empty(); });
          T& result = m_queue.front();
          lock.unlock();
          return result;
      }

      // Called by consumer thread
      const T& front() const
      {
          auto lock = std::unique_lock<std::mutex>(m_mutex);
          m_cvar.wait(lock, [&]{ return !m_queue.empty(); });
          T& result = m_queue.front();
          lock.unlock();
          return result;
      }

      bool empty() const
      {
          auto g = guard(m_mutex);
          return m_queue.front();
      }

      std::size_t size() const
      {
          auto g = guard(m_mutex);
          return  m_queue.size();
      }

  };



  int main(int argc, char* argv[])
  {

      ConcurrentQueue<double> queue;

      // Mutex used for protecting std::cout from frace condition
      std::mutex output_mutex;
      std::atomic_bool finished = false;

      auto consumer_thread = std::thread( [&]
      {
        {
            auto guard = std::lock_guard(output_mutex);
            std::cout << " [CONSUMER] Consumer thread started." << "\n";
        }
          while(true)
          {
              auto value = queue.front();
              {
                  auto guard = std::lock_guard(output_mutex);
                  std::cout << " [CONSUMER] Wait input ..." << "\n";
                  std::cout << " [CONSUMER] Received value = " << value << "\n";
              }
              queue.pop();
              if(!queue.empty() && finished) { break; }
          }
      });

       // Protect cout from race condition
      {
          auto guard = std::lock_guard(output_mutex);
          std::cout << " [TRACE] Start interactive SHELL OK." << "\n";
      }

      // The producer thread is the current one (main thread)
      for(int i = 0; i < 5; i++)
      {
          std::this_thread::sleep_for(1s);
          double x = i * i - 4 * i + 10.0 ;
          {
              auto guard = std::lock_guard(output_mutex);
              std::cout << " [PRODUCER] => Sending number: " << x << "\n";
          }
          queue.push(x);
      }

      finished = true;
      consumer_thread.join();
      return 0;
  }
#+END_SRC

Building: 

#+BEGIN_SRC sh 
  $ clang++ concurrent_queue.cpp -o out.bin -std=c++1z -O0 -Wall -g -lpthread
#+END_SRC

Running:

#+BEGIN_SRC sh 
  $ out.bin  
   [TRACE] Start interactive SHELL OK.
   [CONSUMER] Consumer thread started.
   [PRODUCER] => Sending number: 10
   [CONSUMER] Wait input ...
   [CONSUMER] Received value = 10
   [PRODUCER] => Sending number: 7
   [CONSUMER] Wait input ...
   [CONSUMER] Received value = 7
   [PRODUCER] => Sending number: 6
   [CONSUMER] Wait input ...
   [CONSUMER] Received value = 6
   [PRODUCER] => Sending number: 7
   [CONSUMER] Wait input ...
   [CONSUMER] Received value = 7
   [PRODUCER] => Sending number: 10
   [CONSUMER] Wait input ...
   [CONSUMER] Received value = 10
#+END_SRC

 *Further Reading* 

Other Implementations of Concurrent Blocking Queue: 

  + [[https://github.com/cameron314/concurrentqueue][cameron314/concurrentqueue]] [BEST] - "A fast multi-producer,
    multi-consumer lock-free concurrent queue for C++11"

  + Facebook's Folly - [[https://github.com/facebook/folly/blob/master/folly/docs/ProducerConsumerQueue.md][ProducerConsumerQueue]] (documentation) - Code: [[https://github.com/facebook/folly/blob/master/folly/ProducerConsumerQueue.h][ProducerConsumerQueue.h]]

  + [[https://juanchopanzacpp.wordpress.com/2013/02/26/concurrent-queue-c11/][Concurrent queue  C++11 | Juan's C++ blog]]

  + [[https://www.boost.org/doc/libs/1_70_0/doc/html/thread/sds.html#thread.sds.synchronized_queues][Boost Synchronized Data Structure - Synchronized Queue]]

  + [[https://docs.microsoft.com/en-us/cpp/parallel/concrt/reference/concurrent-queue-class?view=vs-2019][concurrent_queue Class | Microsoft Docs]]

  + [[https://codereview.stackexchange.com/questions/128832/c11-blocking-queue-learning-exercise][c++ - C++11 Blocking Queue learning exercise - Code Review Stack Exchange]]

  + [[https://gist.github.com/thelinked/6997598][C++11 blocking queue using the standard library.  GitHub]]

References: 
  
  + Paper: [[http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p0260r3.html][WG21 - C++ Concurrent Queues]] - Lawerence Crwol, Chris Mysen
    + "Concurrent queues are a fundamental structuring tool for
      concurrent programs. We propose a concurrent queue concept and a
      concrete implementation. We propose a set of communication types
      that enable loosely bound program components to dynamically
      construct and safely share concurrent queues."

  + [[https://www.baeldung.com/java-blocking-queue][Guide to java.util.concurrent.BlockingQueue | Baeldung]]
    + Shows how a concurrent queue works and solves the
      producer/consumer problem.

  + [[https://vorbrodt.blog/2019/02/03/blocking-queue/][Blocking queue  Vorbrodt's C++ Blog]]

